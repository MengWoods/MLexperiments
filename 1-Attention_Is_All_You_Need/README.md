# Attention Is All You Need

## 0. Abstract

**Sequence-to-Sequence (Seq2Seq) models or Transducers**, designed to convert an input seq into an output seq. They can be different lengths and structures.

The core of model is typically and **encoder-decoder architecture**. Often built using recurrent neural networks like LSTMs, GRUs or more recently Transformers with **attention mechanisms**.

- **Encoder**: processes the input seq token by token. Converts the entire seq into a fixed-length representation called the context vector. (Seq -> context vectors)

- **Decoder**: takes the context vector from the encoder and generates the output seq one token at a time.

- The encoder and decoder are built using **Recurrent Neural Network (RNN) layers**, specifically **LSTMs** (Long Short-Term Memory), because they are good at processing sequences.

- **Token**: In NLP, token is the fundamental unit of text that model precesses. Tokens are how language is broken down into small, manageable, numerical representations for a computer to understand and work with.
  - **Tokenization** is the process of deciding how to break the text up to assign numeric IDs. Tokenizer is a set of rules and pre-defined vocabulary that is trained alongside a model, it includes two main steps:
  - 1. Splitting text with tokenization algorithm, modern large language models almost exclusively use **Subword Tokenization** methods. The most common subword techniques are **Byte-Pair Encoding** (BPE) and **WordPiece**.
  - 2. Mapping to numerical IDs. After the raw test is split into tokens, then convert these linguistic units into a format the model's math can process. The sequence of numerical IDs is then fed to the encoder layer of the model for further processing like creating embeddings.
  - **Tokenizer** is a separate preprocessing component acts as the gateway between raw text and the model. The numerical IDs generated by the tokenizer enters model's first layer, which is part of the overall encoder structure: the **embedding layer**.

> [encoder_decoder_architecture.py](./0-Abstract/src/encoder_decoder_architecture.py)

- **Attention mechanism**: It allows the decoder to pay attention to the most relevant parts of the input seq context vector at each step of generating the output. rather than relying on a single fixed size context vector for the whole input.
